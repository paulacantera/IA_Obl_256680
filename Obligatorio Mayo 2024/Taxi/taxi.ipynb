{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Taxi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from taxi_env_extended import TaxiEnvExtended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TaxiEnvExtended()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener la cantidad de estados y acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = env.action_space.n\n",
    "states = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((states, actions))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = np.argmax(Q[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = env.action_space.sample()\n",
    "        # print('explore')\n",
    "    # exploit\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "        # print('exploit')\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemento el algoritmo Q learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes=5000, alpha=0.05, gamma=0.99, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.9995):\n",
    "    Q = np.zeros((states, actions))\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)  # Decaer epsilon\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} - Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entreno el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/5000 - Total Reward: -803, Epsilon: 0.9047921471137096\n",
      "Episode 200/5000 - Total Reward: -623, Epsilon: 0.818648829478636\n",
      "Episode 300/5000 - Total Reward: -686, Epsilon: 0.7407070321560997\n",
      "Episode 400/5000 - Total Reward: -361, Epsilon: 0.6701859060067403\n",
      "Episode 500/5000 - Total Reward: -50, Epsilon: 0.6063789448611848\n",
      "Episode 600/5000 - Total Reward: -133, Epsilon: 0.5486469074854965\n",
      "Episode 700/5000 - Total Reward: -62, Epsilon: 0.4964114134310989\n",
      "Episode 800/5000 - Total Reward: -91, Epsilon: 0.4491491486100748\n",
      "Episode 900/5000 - Total Reward: -43, Epsilon: 0.4063866225452039\n",
      "Episode 1000/5000 - Total Reward: -87, Epsilon: 0.3676954247709635\n",
      "Episode 1100/5000 - Total Reward: 13, Epsilon: 0.33268793286240766\n",
      "Episode 1200/5000 - Total Reward: -44, Epsilon: 0.3010134290933992\n",
      "Episode 1300/5000 - Total Reward: 0, Epsilon: 0.27235458681947705\n",
      "Episode 1400/5000 - Total Reward: -14, Epsilon: 0.24642429138466176\n",
      "Episode 1500/5000 - Total Reward: -6, Epsilon: 0.22296276370290227\n",
      "Episode 1600/5000 - Total Reward: -23, Epsilon: 0.20173495769715546\n",
      "Episode 1700/5000 - Total Reward: 0, Epsilon: 0.18252820552270246\n",
      "Episode 1800/5000 - Total Reward: -4, Epsilon: 0.1651500869836984\n",
      "Episode 1900/5000 - Total Reward: 11, Epsilon: 0.14942650179799613\n",
      "Episode 2000/5000 - Total Reward: 5, Epsilon: 0.1351999253974994\n",
      "Episode 2100/5000 - Total Reward: 3, Epsilon: 0.12232783079001676\n",
      "Episode 2200/5000 - Total Reward: -5, Epsilon: 0.11068126067226178\n",
      "Episode 2300/5000 - Total Reward: 10, Epsilon: 0.10014353548890782\n",
      "Episode 2400/5000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 2500/5000 - Total Reward: 0, Epsilon: 0.1\n",
      "Episode 2600/5000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 2700/5000 - Total Reward: 6, Epsilon: 0.1\n",
      "Episode 2800/5000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 2900/5000 - Total Reward: -1, Epsilon: 0.1\n",
      "Episode 3000/5000 - Total Reward: 6, Epsilon: 0.1\n",
      "Episode 3100/5000 - Total Reward: 13, Epsilon: 0.1\n",
      "Episode 3200/5000 - Total Reward: -14, Epsilon: 0.1\n",
      "Episode 3300/5000 - Total Reward: 3, Epsilon: 0.1\n",
      "Episode 3400/5000 - Total Reward: 13, Epsilon: 0.1\n",
      "Episode 3500/5000 - Total Reward: 5, Epsilon: 0.1\n",
      "Episode 3600/5000 - Total Reward: 7, Epsilon: 0.1\n",
      "Episode 3700/5000 - Total Reward: -16, Epsilon: 0.1\n",
      "Episode 3800/5000 - Total Reward: 5, Epsilon: 0.1\n",
      "Episode 3900/5000 - Total Reward: 5, Epsilon: 0.1\n",
      "Episode 4000/5000 - Total Reward: -6, Epsilon: 0.1\n",
      "Episode 4100/5000 - Total Reward: -7, Epsilon: 0.1\n",
      "Episode 4200/5000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 4300/5000 - Total Reward: 11, Epsilon: 0.1\n",
      "Episode 4400/5000 - Total Reward: 6, Epsilon: 0.1\n",
      "Episode 4500/5000 - Total Reward: 2, Epsilon: 0.1\n",
      "Episode 4600/5000 - Total Reward: -3, Epsilon: 0.1\n",
      "Episode 4700/5000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 4800/5000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 4900/5000 - Total Reward: -6, Epsilon: 0.1\n",
      "Episode 5000/5000 - Total Reward: 2, Epsilon: 0.1\n"
     ]
    }
   ],
   "source": [
    "Q = q_learning(env, num_episodes=5000, alpha=0.15, gamma=0.95, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluo la policy optima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, num_episodes=100):\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    average_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: 7.44\n"
     ]
    }
   ],
   "source": [
    "average_reward = evaluate_policy(env, Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
