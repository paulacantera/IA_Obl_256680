{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta notebook contiene bloques de código útiles para realizar Q-learning en el entorno \"Taxi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from taxi_env_extended import TaxiEnvExtended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TaxiEnvExtended()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener la cantidad de estados y acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = env.action_space.n\n",
    "states = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((states, actions))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de la acción a partir de la tabla Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(state, Q):\n",
    "    action = np.argmax(Q[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon-Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, Q, epsilon=0.1):\n",
    "    explore = np.random.binomial(1, epsilon)\n",
    "    if explore:\n",
    "        action = env.action_space.sample()\n",
    "        # print('explore')\n",
    "    # exploit\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "        # print('exploit')\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de episodio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemento el algoritmo Q learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma, epsilon_start, epsilon_end, epsilon_decay):\n",
    "    Q = np.zeros((states, actions))\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(state, Q, epsilon)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            best_next_action = np.argmax(Q[next_state])# aca en realidad podria usar la funcion de optimal_policy\n",
    "            td_target = reward + gamma * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)  #aca decaigo epsilon\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} - Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entreno el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/10000 - Total Reward: -623, Epsilon: 0.9047921471137096\n",
      "Episode 200/10000 - Total Reward: -623, Epsilon: 0.818648829478636\n",
      "Episode 300/10000 - Total Reward: -464, Epsilon: 0.7407070321560997\n",
      "Episode 400/10000 - Total Reward: -333, Epsilon: 0.6701859060067403\n",
      "Episode 500/10000 - Total Reward: -515, Epsilon: 0.6063789448611848\n",
      "Episode 600/10000 - Total Reward: -220, Epsilon: 0.5486469074854965\n",
      "Episode 700/10000 - Total Reward: -299, Epsilon: 0.4964114134310989\n",
      "Episode 800/10000 - Total Reward: -38, Epsilon: 0.4491491486100748\n",
      "Episode 900/10000 - Total Reward: -99, Epsilon: 0.4063866225452039\n",
      "Episode 1000/10000 - Total Reward: -90, Epsilon: 0.3676954247709635\n",
      "Episode 1100/10000 - Total Reward: -88, Epsilon: 0.33268793286240766\n",
      "Episode 1200/10000 - Total Reward: -30, Epsilon: 0.3010134290933992\n",
      "Episode 1300/10000 - Total Reward: -5, Epsilon: 0.27235458681947705\n",
      "Episode 1400/10000 - Total Reward: -5, Epsilon: 0.24642429138466176\n",
      "Episode 1500/10000 - Total Reward: -1, Epsilon: 0.22296276370290227\n",
      "Episode 1600/10000 - Total Reward: 3, Epsilon: 0.20173495769715546\n",
      "Episode 1700/10000 - Total Reward: -7, Epsilon: 0.18252820552270246\n",
      "Episode 1800/10000 - Total Reward: -15, Epsilon: 0.1651500869836984\n",
      "Episode 1900/10000 - Total Reward: -6, Epsilon: 0.14942650179799613\n",
      "Episode 2000/10000 - Total Reward: -8, Epsilon: 0.1351999253974994\n",
      "Episode 2100/10000 - Total Reward: 8, Epsilon: 0.12232783079001676\n",
      "Episode 2200/10000 - Total Reward: 8, Epsilon: 0.11068126067226178\n",
      "Episode 2300/10000 - Total Reward: 10, Epsilon: 0.10014353548890782\n",
      "Episode 2400/10000 - Total Reward: 0, Epsilon: 0.1\n",
      "Episode 2500/10000 - Total Reward: 12, Epsilon: 0.1\n",
      "Episode 2600/10000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 2700/10000 - Total Reward: -14, Epsilon: 0.1\n",
      "Episode 2800/10000 - Total Reward: 11, Epsilon: 0.1\n",
      "Episode 2900/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 3000/10000 - Total Reward: -2, Epsilon: 0.1\n",
      "Episode 3100/10000 - Total Reward: -4, Epsilon: 0.1\n",
      "Episode 3200/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 3300/10000 - Total Reward: 4, Epsilon: 0.1\n",
      "Episode 3400/10000 - Total Reward: 6, Epsilon: 0.1\n",
      "Episode 3500/10000 - Total Reward: 5, Epsilon: 0.1\n",
      "Episode 3600/10000 - Total Reward: -4, Epsilon: 0.1\n",
      "Episode 3700/10000 - Total Reward: -8, Epsilon: 0.1\n",
      "Episode 3800/10000 - Total Reward: 4, Epsilon: 0.1\n",
      "Episode 3900/10000 - Total Reward: -5, Epsilon: 0.1\n",
      "Episode 4000/10000 - Total Reward: 6, Epsilon: 0.1\n",
      "Episode 4100/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 4200/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 4300/10000 - Total Reward: -10, Epsilon: 0.1\n",
      "Episode 4400/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 4500/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 4600/10000 - Total Reward: -5, Epsilon: 0.1\n",
      "Episode 4700/10000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 4800/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 4900/10000 - Total Reward: 11, Epsilon: 0.1\n",
      "Episode 5000/10000 - Total Reward: -3, Epsilon: 0.1\n",
      "Episode 5100/10000 - Total Reward: 12, Epsilon: 0.1\n",
      "Episode 5200/10000 - Total Reward: -6, Epsilon: 0.1\n",
      "Episode 5300/10000 - Total Reward: -7, Epsilon: 0.1\n",
      "Episode 5400/10000 - Total Reward: 6, Epsilon: 0.1\n",
      "Episode 5500/10000 - Total Reward: 2, Epsilon: 0.1\n",
      "Episode 5600/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 5700/10000 - Total Reward: 11, Epsilon: 0.1\n",
      "Episode 5800/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 5900/10000 - Total Reward: -4, Epsilon: 0.1\n",
      "Episode 6000/10000 - Total Reward: 7, Epsilon: 0.1\n",
      "Episode 6100/10000 - Total Reward: 7, Epsilon: 0.1\n",
      "Episode 6200/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 6300/10000 - Total Reward: -5, Epsilon: 0.1\n",
      "Episode 6400/10000 - Total Reward: 5, Epsilon: 0.1\n",
      "Episode 6500/10000 - Total Reward: -7, Epsilon: 0.1\n",
      "Episode 6600/10000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 6700/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 6800/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 6900/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 7000/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 7100/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 7200/10000 - Total Reward: 4, Epsilon: 0.1\n",
      "Episode 7300/10000 - Total Reward: 14, Epsilon: 0.1\n",
      "Episode 7400/10000 - Total Reward: -7, Epsilon: 0.1\n",
      "Episode 7500/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 7600/10000 - Total Reward: 4, Epsilon: 0.1\n",
      "Episode 7700/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 7800/10000 - Total Reward: 7, Epsilon: 0.1\n",
      "Episode 7900/10000 - Total Reward: -15, Epsilon: 0.1\n",
      "Episode 8000/10000 - Total Reward: 8, Epsilon: 0.1\n",
      "Episode 8100/10000 - Total Reward: -5, Epsilon: 0.1\n",
      "Episode 8200/10000 - Total Reward: -3, Epsilon: 0.1\n",
      "Episode 8300/10000 - Total Reward: -15, Epsilon: 0.1\n",
      "Episode 8400/10000 - Total Reward: 7, Epsilon: 0.1\n",
      "Episode 8500/10000 - Total Reward: 7, Epsilon: 0.1\n",
      "Episode 8600/10000 - Total Reward: 5, Epsilon: 0.1\n",
      "Episode 8700/10000 - Total Reward: -7, Epsilon: 0.1\n",
      "Episode 8800/10000 - Total Reward: 0, Epsilon: 0.1\n",
      "Episode 8900/10000 - Total Reward: -2, Epsilon: 0.1\n",
      "Episode 9000/10000 - Total Reward: 0, Epsilon: 0.1\n",
      "Episode 9100/10000 - Total Reward: -4, Epsilon: 0.1\n",
      "Episode 9200/10000 - Total Reward: 10, Epsilon: 0.1\n",
      "Episode 9300/10000 - Total Reward: 5, Epsilon: 0.1\n",
      "Episode 9400/10000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 9500/10000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 9600/10000 - Total Reward: 5, Epsilon: 0.1\n",
      "Episode 9700/10000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 9800/10000 - Total Reward: 9, Epsilon: 0.1\n",
      "Episode 9900/10000 - Total Reward: 4, Epsilon: 0.1\n",
      "Episode 10000/10000 - Total Reward: -31, Epsilon: 0.1\n"
     ]
    }
   ],
   "source": [
    "Q = q_learning(env, num_episodes=10000, alpha=0.1, gamma=0.95, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluo la policy optima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, num_episodes=100):\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    average_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward over 100 episodes: 8.13\n"
     ]
    }
   ],
   "source": [
    "average_reward = evaluate_policy(env, Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
